{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0bdTBCBjz8T",
        "outputId": "b3246f64-2816-4449-e1af-2bfcf3f28a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 1s (186 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda is already the newest version (12.8.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cuda\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKlDvgzGkQhC",
        "outputId": "1fbfabe9-3980-40ed-a9f9-e79f7b9b327c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2025 NVIDIA Corporation\n",
            "Built on Wed_Jan_15_19:20:09_PST_2025\n",
            "Cuda compilation tools, release 12.8, V12.8.61\n",
            "Build cuda_12.8.r12.8/compiler.35404655_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rblKOP42kW_S",
        "outputId": "96174284-05fb-440d-bd98-43365fb51194"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#define CUDNN_MAJOR 8\n",
            "#define CUDNN_MINOR 9\n",
            "#define CUDNN_PATCHLEVEL 6\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libnpp-dev-11-8\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmbt3lNqlf9t",
        "outputId": "301bee34-576d-4a6e-90ad-fcf388f8c67f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libnpp-dev-11-8 is already the newest version (11.8.0.86-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "***************************************************\n",
        " * code_cuda.cu\n",
        " *\n",
        " * Extended example GPU-based project demonstrating:\n",
        " *   1. Basic vector ops (addition, fill, ReLU, etc.)\n",
        " *   2. Dot product and mat-vector multiply via cuBLAS\n",
        " *   3. Simple feed-forward MLP training (2-layer) with\n",
        " *      nonzero MSE and actual gradient update\n",
        " *   4. Simple cuDNN convolution, activation, pooling, softmax\n",
        " *   5. (Stub) NPP image rotation\n",
        " *\n",
        " * Build (example):\n",
        " *   nvcc code_cuda.cu -o code_cuda -lcudart -lcublas -lcudnn\n",
        " **************************************************/\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cassert>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "\n",
        "// CUDA headers\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cudnn.h>\n",
        "\n",
        "// Some constants, for demonstration\n",
        "#define CHECK_CUDA_ERR(x)  do { if((x) != cudaSuccess) { \\\n",
        "                                fprintf(stderr,\"Error at %s:%d - %s\\n\",__FILE__,__LINE__,cudaGetErrorString(x)); \\\n",
        "                                exit(EXIT_FAILURE);}} while(0)\n",
        "\n",
        "#define CHECK_CUBLAS_ERR(x) do { if((x) != CUBLAS_STATUS_SUCCESS) { \\\n",
        "                                  fprintf(stderr,\"CUBLAS Error at %s:%d\\n\",__FILE__,__LINE__); \\\n",
        "                                  exit(EXIT_FAILURE);}} while(0)\n",
        "\n",
        "#define CHECK_CUDNN_ERR(x) do { cudnnStatus_t status_ = (x); \\\n",
        "                                if (status_ != CUDNN_STATUS_SUCCESS) { \\\n",
        "                                  fprintf(stderr, \"cuDNN Error: %s at %s:%d\\n\", \\\n",
        "                                          cudnnGetErrorString(status_), __FILE__, __LINE__); \\\n",
        "                                  exit(EXIT_FAILURE);}} while(0)\n",
        "\n",
        "//---------------------------------------------\n",
        "// Utility functions\n",
        "//---------------------------------------------\n",
        "static inline float randomFloat(float low = -1.0f, float high = 1.0f)\n",
        "{\n",
        "    float r = static_cast<float>(rand()) / static_cast<float>(RAND_MAX);\n",
        "    return low + r * (high - low);\n",
        "}\n",
        "\n",
        "__global__ void fillArrayKernel(float* arr, int N, float value)\n",
        "{\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if(idx < N) arr[idx] = value;\n",
        "}\n",
        "\n",
        "__global__ void addArraysKernel(const float* a, const float* b, float* c, int N)\n",
        "{\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if(idx < N) c[idx] = a[idx] + b[idx];\n",
        "}\n",
        "\n",
        "__global__ void reluKernel(float* data, int N)\n",
        "{\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if(idx < N) data[idx] = fmaxf(0.0f, data[idx]);\n",
        "}\n",
        "\n",
        "__global__ void reluBackwardKernel(const float* input, const float* gradOutput, float* gradInput, int N)\n",
        "{\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if(idx < N)\n",
        "        gradInput[idx] = (input[idx] > 0.0f) ? gradOutput[idx] : 0.0f;\n",
        "}\n",
        "\n",
        "__global__ void squareKernel(float* data, int N)\n",
        "{\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if(idx < N)\n",
        "    {\n",
        "        float val = data[idx];\n",
        "        data[idx] = 0.5f * val * val; // 1/2 * (val^2)\n",
        "    }\n",
        "}\n",
        "\n",
        "// Wrapper for fill kernel\n",
        "void gpuFillArray(float* d_arr, int N, float value)\n",
        "{\n",
        "    int threads = 256;\n",
        "    int blocks = (N + threads - 1) / threads;\n",
        "    fillArrayKernel<<<blocks, threads>>>(d_arr, N, value);\n",
        "    CHECK_CUDA_ERR(cudaDeviceSynchronize());\n",
        "}\n",
        "\n",
        "// Wrapper for add kernel\n",
        "void gpuAddArrays(const float* d_a, const float* d_b, float* d_c, int N)\n",
        "{\n",
        "    int threads = 256;\n",
        "    int blocks = (N + threads - 1) / threads;\n",
        "    addArraysKernel<<<blocks, threads>>>(d_a, d_b, d_c, N);\n",
        "    CHECK_CUDA_ERR(cudaDeviceSynchronize());\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// cuBLAS matrix-multiply: C = A * B\n",
        "// A: (M x K), B: (K x N), C: (M x N)\n",
        "//---------------------------------------------\n",
        "void matMulCuBLAS(cublasHandle_t handle,\n",
        "                  const float* d_A, const float* d_B, float* d_C,\n",
        "                  int M, int N, int K)\n",
        "{\n",
        "    const float alpha = 1.0f;\n",
        "    const float beta  = 0.0f;\n",
        "    CHECK_CUBLAS_ERR(\n",
        "        cublasSgemm(handle,\n",
        "                    CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                    M, N, K,\n",
        "                    &alpha,\n",
        "                    d_A, M,\n",
        "                    d_B, K,\n",
        "                    &beta,\n",
        "                    d_C, M)\n",
        "    );\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Fully connected: y = W*x + b\n",
        "// W: (outDim x inDim), x: (inDim x 1)\n",
        "//---------------------------------------------\n",
        "void fullyConnectedLayer(cublasHandle_t handle,\n",
        "                         const float* d_W,\n",
        "                         /*const float* d_b,*/ // b addition is separate\n",
        "                         const float* d_x,\n",
        "                         float* d_y,\n",
        "                         int inDim,\n",
        "                         int outDim)\n",
        "{\n",
        "    // y = W*x (we'll add bias with a separate kernel or approach)\n",
        "    matMulCuBLAS(handle, d_W, d_x, d_y, outDim, 1, inDim);\n",
        "}\n",
        "\n",
        "// Add bias (per-element)\n",
        "__global__ void addBiasKernel(float* d_y, const float* d_b, int outDim)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(idx < outDim)\n",
        "        d_y[idx] += d_b[idx];\n",
        "}\n",
        "\n",
        "void fullyConnectedLayerAddBias(const float* d_b, float* d_y, int outDim)\n",
        "{\n",
        "    int threads = 256;\n",
        "    int blocks = (outDim + threads - 1) / threads;\n",
        "    addBiasKernel<<<blocks, threads>>>(d_y, d_b, outDim);\n",
        "    CHECK_CUDA_ERR(cudaDeviceSynchronize());\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Backprop for fully connected layer\n",
        "// gradOut: gradient wrt output (outDim x 1)\n",
        "// X: input (inDim x 1)\n",
        "// W: (outDim x inDim)\n",
        "//---------------------------------------------\n",
        "void backpropFullyConnected(cublasHandle_t handle,\n",
        "                            const float* d_gradOut,\n",
        "                            const float* d_X,\n",
        "                            const float* d_W,\n",
        "                            float* d_gradW,\n",
        "                            float* d_gradB,\n",
        "                            float* d_gradX,\n",
        "                            int inDim, int outDim)\n",
        "{\n",
        "    // gradW = gradOut * X^T => (outDim x 1)*(1 x inDim) => (outDim x inDim)\n",
        "    {\n",
        "        const float alpha = 1.0f;\n",
        "        const float beta  = 0.0f;\n",
        "        CHECK_CUBLAS_ERR(\n",
        "            cublasSgemm(handle,\n",
        "                        CUBLAS_OP_N, CUBLAS_OP_T,\n",
        "                        outDim, inDim, 1,\n",
        "                        &alpha,\n",
        "                        d_gradOut, outDim,\n",
        "                        d_X, inDim,\n",
        "                        &beta,\n",
        "                        d_gradW, outDim)\n",
        "        );\n",
        "    }\n",
        "\n",
        "    // gradB = gradOut (one value per output neuron in this single-sample case)\n",
        "    CHECK_CUDA_ERR(cudaMemcpy(d_gradB, d_gradOut, outDim*sizeof(float), cudaMemcpyDeviceToDevice));\n",
        "\n",
        "    // gradX = W^T * gradOut => (inDim x outDim)*(outDim x 1) => (inDim x 1)\n",
        "    {\n",
        "        const float alpha = 1.0f;\n",
        "        const float beta  = 0.0f;\n",
        "        CHECK_CUBLAS_ERR(\n",
        "            cublasSgemm(handle,\n",
        "                        CUBLAS_OP_T, CUBLAS_OP_N,\n",
        "                        inDim, 1, outDim,\n",
        "                        &alpha,\n",
        "                        d_W, outDim,\n",
        "                        d_gradOut, outDim,\n",
        "                        &beta,\n",
        "                        d_gradX, inDim)\n",
        "        );\n",
        "    }\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Extra cuDNN demos: Convolution, ReLU, Pooling, Softmax\n",
        "//---------------------------------------------\n",
        "void cudnnConvolutionExample(cudnnHandle_t cudnn)\n",
        "{\n",
        "    std::cout << \"[INFO] Running a sample cuDNN Convolution...\\n\";\n",
        "\n",
        "    cudnnTensorDescriptor_t inDesc, outDesc;\n",
        "    cudnnFilterDescriptor_t filtDesc;\n",
        "    cudnnConvolutionDescriptor_t convDesc;\n",
        "\n",
        "    CHECK_CUDNN_ERR( cudnnCreateTensorDescriptor(&inDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnCreateTensorDescriptor(&outDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnCreateFilterDescriptor(&filtDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnCreateConvolutionDescriptor(&convDesc) );\n",
        "\n",
        "    // For a 28x28 single-channel example\n",
        "    CHECK_CUDNN_ERR( cudnnSetTensor4dDescriptor(inDesc, CUDNN_TENSOR_NCHW,\n",
        "                                                CUDNN_DATA_FLOAT,\n",
        "                                                1, 1, 28, 28) );\n",
        "\n",
        "    CHECK_CUDNN_ERR( cudnnSetTensor4dDescriptor(outDesc, CUDNN_TENSOR_NCHW,\n",
        "                                                CUDNN_DATA_FLOAT,\n",
        "                                                1, 1, 24, 24) );\n",
        "\n",
        "    CHECK_CUDNN_ERR( cudnnSetFilter4dDescriptor(filtDesc,\n",
        "                                                CUDNN_DATA_FLOAT,\n",
        "                                                CUDNN_TENSOR_NCHW,\n",
        "                                                1, 1, 5, 5) );\n",
        "\n",
        "    CHECK_CUDNN_ERR( cudnnSetConvolution2dDescriptor(convDesc,\n",
        "                                                     0, 0,\n",
        "                                                     1, 1,\n",
        "                                                     1, 1,\n",
        "                                                     CUDNN_CROSS_CORRELATION,\n",
        "                                                     CUDNN_DATA_FLOAT) );\n",
        "\n",
        "    // We'll pick a known forward algo\n",
        "    cudnnConvolutionFwdAlgo_t algo = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;\n",
        "\n",
        "    size_t workspaceBytes = 0;\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnGetConvolutionForwardWorkspaceSize(\n",
        "            cudnn, inDesc, filtDesc, convDesc, outDesc, algo, &workspaceBytes)\n",
        "    );\n",
        "\n",
        "    void* d_workspace = nullptr;\n",
        "    if (workspaceBytes > 0)\n",
        "        CHECK_CUDA_ERR( cudaMalloc(&d_workspace, workspaceBytes) );\n",
        "\n",
        "    float *d_input = nullptr, *d_filter = nullptr, *d_output = nullptr;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_input,  1*1*28*28*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_filter, 1*1*5*5*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_output, 1*1*24*24*sizeof(float)) );\n",
        "\n",
        "    // fill input/filter with some constants\n",
        "    gpuFillArray(d_input, 1*1*28*28, 0.1f);\n",
        "    gpuFillArray(d_filter, 1*1*5*5,  0.2f);\n",
        "\n",
        "    const float alpha = 1.0f, beta = 0.0f;\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnConvolutionForward(cudnn,\n",
        "                                &alpha,\n",
        "                                inDesc, d_input,\n",
        "                                filtDesc, d_filter,\n",
        "                                convDesc, algo,\n",
        "                                d_workspace, workspaceBytes,\n",
        "                                &beta,\n",
        "                                outDesc, d_output)\n",
        "    );\n",
        "\n",
        "    // Cleanup\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyConvolutionDescriptor(convDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyFilterDescriptor(filtDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyTensorDescriptor(inDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyTensorDescriptor(outDesc) );\n",
        "    if(d_workspace) cudaFree(d_workspace);\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_filter);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    std::cout << \"[INFO] cuDNN Convolution example complete.\\n\";\n",
        "}\n",
        "\n",
        "void cudnnReluExample(cudnnHandle_t cudnn)\n",
        "{\n",
        "    std::cout << \"[INFO] Running a sample cuDNN ReLU...\\n\";\n",
        "\n",
        "    cudnnTensorDescriptor_t desc;\n",
        "    cudnnActivationDescriptor_t actDesc;\n",
        "    CHECK_CUDNN_ERR( cudnnCreateTensorDescriptor(&desc) );\n",
        "    CHECK_CUDNN_ERR( cudnnCreateActivationDescriptor(&actDesc) );\n",
        "\n",
        "    CHECK_CUDNN_ERR( cudnnSetTensor4dDescriptor(desc,\n",
        "                                                CUDNN_TENSOR_NCHW,\n",
        "                                                CUDNN_DATA_FLOAT,\n",
        "                                                1, 10, 1, 1) );\n",
        "\n",
        "    CHECK_CUDNN_ERR( cudnnSetActivationDescriptor(actDesc,\n",
        "                                                  CUDNN_ACTIVATION_RELU,\n",
        "                                                  CUDNN_PROPAGATE_NAN,\n",
        "                                                  0.0) );\n",
        "\n",
        "    float *d_input = nullptr, *d_output = nullptr;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_input,  10*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_output, 10*sizeof(float)) );\n",
        "\n",
        "    // Fill with negative to see ReLU effect\n",
        "    gpuFillArray(d_input, 10, -0.5f);\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnActivationForward(cudnn,\n",
        "                               actDesc,\n",
        "                               &alpha,\n",
        "                               desc, d_input,\n",
        "                               &beta,\n",
        "                               desc, d_output)\n",
        "    );\n",
        "\n",
        "    // Cleanup\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyActivationDescriptor(actDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyTensorDescriptor(desc) );\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    std::cout << \"[INFO] cuDNN ReLU example complete.\\n\";\n",
        "}\n",
        "\n",
        "void cudnnPoolingExample(cudnnHandle_t cudnn)\n",
        "{\n",
        "    std::cout << \"[INFO] Running a sample cuDNN Pooling (Max Pool)...\\n\";\n",
        "\n",
        "    cudnnTensorDescriptor_t inDesc, outDesc;\n",
        "    cudnnPoolingDescriptor_t poolDesc;\n",
        "    CHECK_CUDNN_ERR( cudnnCreateTensorDescriptor(&inDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnCreateTensorDescriptor(&outDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnCreatePoolingDescriptor(&poolDesc) );\n",
        "\n",
        "    // 1x1x6x6 input\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnSetTensor4dDescriptor(inDesc, CUDNN_TENSOR_NCHW,\n",
        "                                   CUDNN_DATA_FLOAT, 1, 1, 6, 6) );\n",
        "\n",
        "    // 2x2 kernel, stride 2 => output 1x1x3x3\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnSetTensor4dDescriptor(outDesc, CUDNN_TENSOR_NCHW,\n",
        "                                   CUDNN_DATA_FLOAT, 1, 1, 3, 3) );\n",
        "\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnSetPooling2dDescriptor(poolDesc,\n",
        "                                    CUDNN_POOLING_MAX,\n",
        "                                    CUDNN_PROPAGATE_NAN,\n",
        "                                    2, 2,  // window\n",
        "                                    0, 0,  // pad\n",
        "                                    2, 2)  // stride\n",
        "    );\n",
        "\n",
        "    float *d_input = nullptr, *d_output = nullptr;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_input,  6*6*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_output, 3*3*sizeof(float)) );\n",
        "\n",
        "    // Fill input with random data\n",
        "    std::vector<float> h_in(36);\n",
        "    for (int i = 0; i < 36; i++){\n",
        "        h_in[i] = (float)(rand() % 100);\n",
        "    }\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_input, h_in.data(), 36*sizeof(float),\n",
        "                               cudaMemcpyHostToDevice) );\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnPoolingForward(cudnn,\n",
        "                            poolDesc,\n",
        "                            &alpha,\n",
        "                            inDesc, d_input,\n",
        "                            &beta,\n",
        "                            outDesc, d_output)\n",
        "    );\n",
        "\n",
        "    // Download result\n",
        "    std::vector<float> h_out(9);\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(h_out.data(), d_output, 9*sizeof(float),\n",
        "                               cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    std::cout << \"[INFO] Pooled Output (3x3):\\n\";\n",
        "    for(int i=0; i<9; i++){\n",
        "        std::cout << std::setw(5) << h_out[i]\n",
        "                  << ((i%3==2) ? \"\\n\" : \" \");\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    CHECK_CUDA_ERR( cudaFree(d_input) );\n",
        "    CHECK_CUDA_ERR( cudaFree(d_output) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyPoolingDescriptor(poolDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyTensorDescriptor(inDesc) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyTensorDescriptor(outDesc) );\n",
        "\n",
        "    std::cout << \"[INFO] cuDNN Pooling example complete.\\n\";\n",
        "}\n",
        "\n",
        "void cudnnSoftmaxExample(cudnnHandle_t cudnn)\n",
        "{\n",
        "    std::cout << \"[INFO] Running a sample cuDNN Softmax...\\n\";\n",
        "    cudnnTensorDescriptor_t desc;\n",
        "    CHECK_CUDNN_ERR( cudnnCreateTensorDescriptor(&desc) );\n",
        "\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnSetTensor4dDescriptor(desc,\n",
        "                                   CUDNN_TENSOR_NCHW,\n",
        "                                   CUDNN_DATA_FLOAT,\n",
        "                                   1, 5, 1, 1) );\n",
        "\n",
        "    float *d_input = nullptr, *d_output = nullptr;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_input,  5*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_output, 5*sizeof(float)) );\n",
        "\n",
        "    // Fill input with random values\n",
        "    std::vector<float> h_in(5);\n",
        "    for(int i=0; i<5; i++){\n",
        "        h_in[i] = randomFloat(-2.0f, 2.0f);\n",
        "    }\n",
        "\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_input, h_in.data(),\n",
        "                               5*sizeof(float),\n",
        "                               cudaMemcpyHostToDevice) );\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "    CHECK_CUDNN_ERR(\n",
        "        cudnnSoftmaxForward(cudnn,\n",
        "                            CUDNN_SOFTMAX_ACCURATE,  // or CUDNN_SOFTMAX_FAST\n",
        "                            CUDNN_SOFTMAX_MODE_INSTANCE,\n",
        "                            &alpha,\n",
        "                            desc, d_input,\n",
        "                            &beta,\n",
        "                            desc, d_output)\n",
        "    );\n",
        "\n",
        "    // Retrieve and print output\n",
        "    std::vector<float> h_out(5);\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(h_out.data(), d_output, 5*sizeof(float),\n",
        "                               cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    std::cout << \"[INFO] Softmax Input -> Output:\\n\";\n",
        "    for(int i=0; i<5; i++){\n",
        "        std::cout << \"  \" << h_in[i] << \" -> \" << h_out[i] << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    CHECK_CUDA_ERR( cudaFree(d_input) );\n",
        "    CHECK_CUDA_ERR( cudaFree(d_output) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroyTensorDescriptor(desc) );\n",
        "\n",
        "    std::cout << \"[INFO] cuDNN Softmax example complete.\\n\";\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Stub for NPP image rotation\n",
        "//---------------------------------------------\n",
        "void nppImageRotationExample(const std::string& filename)\n",
        "{\n",
        "    // Stub only. No actual NPP calls here.\n",
        "    std::cout << \"[INFO] (Stub) NPP Image Rotation of \" << filename << \"\\n\";\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// CPU helper for random init\n",
        "//---------------------------------------------\n",
        "void cpuRandomInit(float* d_arr, int N, float low=-1.0f, float high=1.0f)\n",
        "{\n",
        "    std::vector<float> hostVec(N);\n",
        "    for(int i=0; i<N; i++){\n",
        "        hostVec[i] = randomFloat(low, high);\n",
        "    }\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_arr, hostVec.data(),\n",
        "                               N*sizeof(float),\n",
        "                               cudaMemcpyHostToDevice) );\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Simple NN training (2-layer MLP) with actual\n",
        "// gradient updates so the MSE won't stay at 0.\n",
        "//---------------------------------------------\n",
        "void simpleNeuralNetworkTraining(cublasHandle_t cublas,\n",
        "                                 int inputSize,\n",
        "                                 int hiddenSize,\n",
        "                                 int outputSize,\n",
        "                                 int iterations = 5)\n",
        "{\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    std::cout << \"[INFO] Starting Simple NN Training (\"\n",
        "              << iterations << \" iterations)...\\n\";\n",
        "\n",
        "    // 1) Allocate weights & biases\n",
        "    float *d_W1, *d_b1, *d_W2, *d_b2;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_W1, hiddenSize * inputSize * sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_b1, hiddenSize * sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_W2, outputSize * hiddenSize * sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_b2, outputSize * sizeof(float)) );\n",
        "\n",
        "    // 2) Initialize them in [-0.5, 0.5], for example\n",
        "    cpuRandomInit(d_W1, hiddenSize*inputSize, -0.5f, 0.5f);\n",
        "    cpuRandomInit(d_b1, hiddenSize, -0.5f, 0.5f);\n",
        "    cpuRandomInit(d_W2, outputSize*hiddenSize, -0.5f, 0.5f);\n",
        "    cpuRandomInit(d_b2, outputSize, -0.5f, 0.5f);\n",
        "\n",
        "    // 3) Allocate input & ground truth (single sample for demo)\n",
        "    float *d_x, *d_yGT;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_x, inputSize * sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_yGT, outputSize * sizeof(float)) );\n",
        "\n",
        "    // Input in [0,1]\n",
        "    cpuRandomInit(d_x, inputSize, 0.0f, 1.0f);\n",
        "\n",
        "    // Ground truth: all zeros except 1 for the first dimension\n",
        "    gpuFillArray(d_yGT, outputSize, 0.0f);\n",
        "    float oneVal = 1.0f;\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_yGT, &oneVal,\n",
        "                               sizeof(float),\n",
        "                               cudaMemcpyHostToDevice) );\n",
        "\n",
        "    // Prepare device arrays for intermediate results\n",
        "    float *d_h1, *d_h2;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_h1, hiddenSize*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_h2, outputSize*sizeof(float)) );\n",
        "\n",
        "    float *d_lossVec;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_lossVec, outputSize*sizeof(float)) );\n",
        "\n",
        "    float *d_gradOut2;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradOut2, outputSize*sizeof(float)) );\n",
        "\n",
        "    float *d_gradW2, *d_gradB2, *d_gradH1;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradW2, outputSize*hiddenSize*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradB2, outputSize*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradH1, hiddenSize*sizeof(float)) );\n",
        "\n",
        "    float *d_gradW1, *d_gradB1, *d_gradX;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradW1, hiddenSize*inputSize*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradB1, hiddenSize*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_gradX, inputSize*sizeof(float)) );\n",
        "\n",
        "    // Learning rate\n",
        "    const float learningRate = 0.01f;\n",
        "    const float negLR = -learningRate;\n",
        "\n",
        "    // 4) Training loop\n",
        "    for(int iter=0; iter<iterations; iter++)\n",
        "    {\n",
        "        //---------------------------------\n",
        "        // Forward pass: fc1 -> add bias -> ReLU -> fc2 -> add bias\n",
        "        //---------------------------------\n",
        "        // FC1\n",
        "        fullyConnectedLayer(cublas, d_W1, d_x, d_h1, inputSize, hiddenSize);\n",
        "        fullyConnectedLayerAddBias(d_b1, d_h1, hiddenSize);\n",
        "\n",
        "        // ReLU\n",
        "        {\n",
        "            int threads = 256;\n",
        "            int blocks = (hiddenSize + threads - 1) / threads;\n",
        "            reluKernel<<<blocks, threads>>>(d_h1, hiddenSize);\n",
        "            CHECK_CUDA_ERR(cudaDeviceSynchronize());\n",
        "        }\n",
        "\n",
        "        // FC2\n",
        "        fullyConnectedLayer(cublas, d_W2, d_h1, d_h2, hiddenSize, outputSize);\n",
        "        fullyConnectedLayerAddBias(d_b2, d_h2, outputSize);\n",
        "\n",
        "        //---------------------------------\n",
        "        // Compute MSE Loss = 0.5 * sum((h2 - yGT)^2)\n",
        "        // We'll do (h2 - yGT) in d_lossVec, then square each element * 0.5\n",
        "        //---------------------------------\n",
        "        // Step 1: d_lossVec = h2\n",
        "        CHECK_CUBLAS_ERR( cublasScopy(cublas, outputSize, d_h2, 1, d_lossVec, 1) );\n",
        "        // Step 2: d_lossVec -= yGT\n",
        "        const float alphaNegOne = -1.0f;\n",
        "        CHECK_CUBLAS_ERR( cublasSaxpy(cublas, outputSize, &alphaNegOne, d_yGT, 1, d_lossVec, 1) );\n",
        "        // Step 3: square each element => 0.5 * val^2\n",
        "        {\n",
        "            int threads = 256;\n",
        "            int blocks = (outputSize + threads - 1) / threads;\n",
        "            squareKernel<<<blocks, threads>>>(d_lossVec, outputSize);\n",
        "            CHECK_CUDA_ERR(cudaDeviceSynchronize());\n",
        "        }\n",
        "        // Sum on CPU for printing\n",
        "        std::vector<float> h_lossVec(outputSize);\n",
        "        CHECK_CUDA_ERR( cudaMemcpy(h_lossVec.data(), d_lossVec, outputSize*sizeof(float),\n",
        "                                   cudaMemcpyDeviceToHost) );\n",
        "        float lossSum = 0.f;\n",
        "        for(float v : h_lossVec) lossSum += v;\n",
        "\n",
        "        //---------------------------------\n",
        "        // Backprop\n",
        "        // gradOut2 = (h2 - yGT)\n",
        "        //---------------------------------\n",
        "        CHECK_CUBLAS_ERR( cublasScopy(cublas, outputSize, d_h2, 1, d_gradOut2, 1) );\n",
        "        CHECK_CUBLAS_ERR( cublasSaxpy(cublas, outputSize, &alphaNegOne, d_yGT, 1, d_gradOut2, 1) );\n",
        "\n",
        "        // backprop fc2\n",
        "        backpropFullyConnected(cublas, d_gradOut2, d_h1, d_W2,\n",
        "                               d_gradW2, d_gradB2, d_gradH1,\n",
        "                               hiddenSize, outputSize);\n",
        "\n",
        "        // backprop ReLU\n",
        "        {\n",
        "            int threads = 256;\n",
        "            int blocks = (hiddenSize + threads - 1) / threads;\n",
        "            reluBackwardKernel<<<blocks, threads>>>(d_h1, d_gradH1, d_gradH1, hiddenSize);\n",
        "            CHECK_CUDA_ERR(cudaDeviceSynchronize());\n",
        "        }\n",
        "\n",
        "        // backprop fc1\n",
        "        backpropFullyConnected(cublas, d_gradH1, d_x, d_W1,\n",
        "                               d_gradW1, d_gradB1, d_gradX,\n",
        "                               inputSize, hiddenSize);\n",
        "\n",
        "        //---------------------------------\n",
        "        // Gradient descent update\n",
        "        // W2 -= LR * d_gradW2, etc.\n",
        "        //---------------------------------\n",
        "        CHECK_CUBLAS_ERR( cublasSaxpy(cublas, outputSize*hiddenSize,\n",
        "                                       &negLR, d_gradW2, 1, d_W2, 1) );\n",
        "        CHECK_CUBLAS_ERR( cublasSaxpy(cublas, outputSize,\n",
        "                                       &negLR, d_gradB2, 1, d_b2, 1) );\n",
        "        CHECK_CUBLAS_ERR( cublasSaxpy(cublas, hiddenSize*inputSize,\n",
        "                                       &negLR, d_gradW1, 1, d_W1, 1) );\n",
        "        CHECK_CUBLAS_ERR( cublasSaxpy(cublas, hiddenSize,\n",
        "                                       &negLR, d_gradB1, 1, d_b1, 1) );\n",
        "\n",
        "        //---------------------------------\n",
        "        // Print iteration info\n",
        "        //---------------------------------\n",
        "        std::cout << \"  [Iteration \" << (iter+1)\n",
        "                  << \"] MSE Loss = \" << lossSum << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_W1);\n",
        "    cudaFree(d_b1);\n",
        "    cudaFree(d_W2);\n",
        "    cudaFree(d_b2);\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_yGT);\n",
        "    cudaFree(d_h1);\n",
        "    cudaFree(d_h2);\n",
        "    cudaFree(d_lossVec);\n",
        "    cudaFree(d_gradOut2);\n",
        "    cudaFree(d_gradW2);\n",
        "    cudaFree(d_gradB2);\n",
        "    cudaFree(d_gradH1);\n",
        "    cudaFree(d_gradW1);\n",
        "    cudaFree(d_gradB1);\n",
        "    cudaFree(d_gradX);\n",
        "\n",
        "    std::cout << \"[INFO] Finished Simple NN Training Example.\\n\";\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Additional example: Dot product (cublasSdot)\n",
        "//---------------------------------------------\n",
        "void cublasDotProductExample(cublasHandle_t handle)\n",
        "{\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    std::cout << \"[INFO] Running a cuBLAS Dot Product example...\\n\";\n",
        "    int N = 10;\n",
        "    float *d_a, *d_b;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_a, N*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_b, N*sizeof(float)) );\n",
        "\n",
        "    // Fill with some values\n",
        "    std::vector<float> h_a(N), h_b(N);\n",
        "    for(int i=0; i<N; i++){\n",
        "        h_a[i] = float(i+1); // 1,2,3,...\n",
        "        h_b[i] = 2.0f;       // all 2\n",
        "    }\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_a, h_a.data(), N*sizeof(float), cudaMemcpyHostToDevice) );\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_b, h_b.data(), N*sizeof(float), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    float result = 0.0f;\n",
        "    CHECK_CUBLAS_ERR( cublasSdot(handle, N, d_a, 1, d_b, 1, &result) );\n",
        "\n",
        "    std::cout << \"  Dot Product of [1..10] and [2..2] = \" << result\n",
        "              << \" (expected 110)\\n\";\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    std::cout << \"[INFO] cuBLAS Dot Product example complete.\\n\";\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// Additional example: Matrix-Vector multiply (cublasSgemv)\n",
        "//---------------------------------------------\n",
        "void cublasMatVecExample(cublasHandle_t handle)\n",
        "{\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    std::cout << \"[INFO] Running a cuBLAS Matrix-Vector example (SGEMV)...\\n\";\n",
        "\n",
        "    // Let A be 3x3, x be 3x1 => y = A*x => 3x1\n",
        "    float A[9] = {1.f,2.f,3.f,\n",
        "                  4.f,5.f,6.f,\n",
        "                  7.f,8.f,9.f};\n",
        "    float x[3] = {1.f, 2.f, 3.f};\n",
        "    float *d_A, *d_x, *d_y;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_A, 9*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_x, 3*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_y, 3*sizeof(float)) );\n",
        "\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_A, A, 9*sizeof(float), cudaMemcpyHostToDevice) );\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(d_x, x, 3*sizeof(float), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    // Zero out d_y\n",
        "    gpuFillArray(d_y, 3, 0.0f);\n",
        "\n",
        "    float alpha = 1.f, beta = 0.f;\n",
        "    CHECK_CUBLAS_ERR(\n",
        "        cublasSgemv(handle, CUBLAS_OP_N,\n",
        "                    3, 3,\n",
        "                    &alpha,\n",
        "                    d_A, 3,\n",
        "                    d_x, 1,\n",
        "                    &beta,\n",
        "                    d_y, 1)\n",
        "    );\n",
        "\n",
        "    // Retrieve result\n",
        "    float y[3];\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(y, d_y, 3*sizeof(float), cudaMemcpyDeviceToHost) );\n",
        "    std::cout << \"  [1 2 3; 4 5 6; 7 8 9] * [1; 2; 3] = [\"\n",
        "              << y[0] << \" \" << y[1] << \" \" << y[2] << \"]^T\\n\";\n",
        "    // should be [14 32 50]^T\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_y);\n",
        "\n",
        "    std::cout << \"[INFO] cuBLAS SGEMV example complete.\\n\";\n",
        "}\n",
        "\n",
        "//---------------------------------------------\n",
        "// MAIN\n",
        "//---------------------------------------------\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    srand((unsigned)time(nullptr));\n",
        "    CHECK_CUDA_ERR( cudaSetDevice(0) );\n",
        "\n",
        "    // Create cuBLAS handle\n",
        "    cublasHandle_t cublas;\n",
        "    CHECK_CUBLAS_ERR( cublasCreate(&cublas) );\n",
        "\n",
        "    // Create cuDNN handle\n",
        "    cudnnHandle_t cudnn;\n",
        "    CHECK_CUDNN_ERR( cudnnCreate(&cudnn) );\n",
        "\n",
        "    // 1) Basic array ops\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    std::cout << \"[INFO] Basic Array Ops Demo\\n\";\n",
        "    int N = 1000;\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_a, N*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_b, N*sizeof(float)) );\n",
        "    CHECK_CUDA_ERR( cudaMalloc(&d_c, N*sizeof(float)) );\n",
        "\n",
        "    gpuFillArray(d_a, N, 1.0f);\n",
        "    gpuFillArray(d_b, N, 2.0f);\n",
        "    gpuAddArrays(d_a, d_b, d_c, N);\n",
        "\n",
        "    // Download & check\n",
        "    std::vector<float> h_c(N);\n",
        "    CHECK_CUDA_ERR( cudaMemcpy(h_c.data(), d_c, N*sizeof(float), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    float sumCheck = 0.0f;\n",
        "    for(int i=0; i<N; i++){\n",
        "        sumCheck += h_c[i];\n",
        "    }\n",
        "    std::cout << \"[INFO] Sum of c after add: \" << sumCheck\n",
        "              << \" (expected ~3000 if N=1000)\\n\";\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    // 2) Dot product with cuBLAS\n",
        "    cublasDotProductExample(cublas);\n",
        "\n",
        "    // 3) Matrix-vector multiply with cuBLAS\n",
        "    cublasMatVecExample(cublas);\n",
        "\n",
        "    // 4) cuDNN Convolution\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    cudnnConvolutionExample(cudnn);\n",
        "\n",
        "    // 5) cuDNN ReLU\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    cudnnReluExample(cudnn);\n",
        "\n",
        "    // 6) cuDNN Pooling\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    cudnnPoolingExample(cudnn);\n",
        "\n",
        "    // 7) cuDNN Softmax\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    cudnnSoftmaxExample(cudnn);\n",
        "\n",
        "    // 8) NPP image rotation (stub)\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    nppImageRotationExample(\"input.png\");\n",
        "\n",
        "    // 9) Simple NN training with multiple iterations + gradient update\n",
        "    simpleNeuralNetworkTraining(cublas, /*inputSize=*/128,\n",
        "                                           /*hiddenSize=*/64,\n",
        "                                           /*outputSize=*/10,\n",
        "                                           /*iterations=*/5);\n",
        "\n",
        "    // Cleanup handles\n",
        "    CHECK_CUBLAS_ERR( cublasDestroy(cublas) );\n",
        "    CHECK_CUDNN_ERR( cudnnDestroy(cudnn) );\n",
        "\n",
        "    std::cout << \"----------------------------------\\n\";\n",
        "    std::cout << \"[INFO] Program finished successfully.\\n\";\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI4LSD4HkX-5",
        "outputId": "af84069a-4657-4c56-a171-8e7c9a994637"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting code_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Write the file (the code above) in a notebook cell with %%writefile code_cuda.cu\n",
        "# 2) Compile:\n",
        "!nvcc code_cuda.cu -o code_cuda -lcublas -lcudnn\n",
        "# 3) Run:\n",
        "!./code_cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpOy-qQBxmsJ",
        "outputId": "ad68cef8-d398-4ff6-b334-1edbbb91f122"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "----------------------------------\n",
            "[INFO] Basic Array Ops Demo\n",
            "[INFO] Sum of c after add: 0 (expected ~3000 if N=1000)\n",
            "----------------------------------\n",
            "[INFO] Running a cuBLAS Dot Product example...\n",
            "  Dot Product of [1..10] and [2..2] = 110 (expected 110)\n",
            "[INFO] cuBLAS Dot Product example complete.\n",
            "----------------------------------\n",
            "[INFO] Running a cuBLAS Matrix-Vector example (SGEMV)...\n",
            "  [1 2 3; 4 5 6; 7 8 9] * [1; 2; 3] = [30 36 42]^T\n",
            "[INFO] cuBLAS SGEMV example complete.\n",
            "----------------------------------\n",
            "[INFO] Running a sample cuDNN Convolution...\n",
            "[INFO] cuDNN Convolution example complete.\n",
            "----------------------------------\n",
            "[INFO] Running a sample cuDNN ReLU...\n",
            "[INFO] cuDNN ReLU example complete.\n",
            "----------------------------------\n",
            "[INFO] Running a sample cuDNN Pooling (Max Pool)...\n",
            "[INFO] Pooled Output (3x3):\n",
            "   96    82    85\n",
            "   90    65    78\n",
            "   92    94    80\n",
            "[INFO] cuDNN Pooling example complete.\n",
            "----------------------------------\n",
            "[INFO] Running a sample cuDNN Softmax...\n",
            "[INFO] Softmax Input -> Output:\n",
            "  0.63331 -> 0.101178\n",
            "  0.0225978 -> 0.0549362\n",
            "  1.93278 -> 0.371057\n",
            "  0.625066 -> 0.100348\n",
            "  1.93661 -> 0.372481\n",
            "[INFO] cuDNN Softmax example complete.\n",
            "----------------------------------\n",
            "[INFO] (Stub) NPP Image Rotation of input.png\n",
            "----------------------------------\n",
            "[INFO] Starting Simple NN Training (5 iterations)...\n",
            "  [Iteration 1] MSE Loss = -6.90843\n",
            "  [Iteration 2] MSE Loss = 12.7059\n",
            "  [Iteration 3] MSE Loss = 39.305\n",
            "  [Iteration 4] MSE Loss = -12.3355\n",
            "  [Iteration 5] MSE Loss = 424.693\n",
            "[INFO] Finished Simple NN Training Example.\n",
            "----------------------------------\n",
            "[INFO] Program finished successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "or_kcAIqrdra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}